{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "chaxueus2"
		},
		"chaxuamleus_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'chaxuamleus'"
		},
		"chaxueus2-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'chaxueus2-WorkspaceDefaultSqlServer'"
		},
		"yifsoeus2euap-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'yifsoeus2euap-WorkspaceDefaultSqlServer'"
		},
		"AzureKeyVault1_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://yifso-akv.vault.azure.net/"
		},
		"bing-covid-19-data_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'bing-covid-19-data'"
		},
		"chaxuamleus_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"chaxuamleus_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "chaxu-test"
		},
		"chaxuamleus_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "a82dd51d-9e81-4835-ae78-dae1b3ffae64"
		},
		"chaxuamleus_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"chaxueus2-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://chaxustorageeus.dfs.core.windows.net"
		},
		"yifsoeus2euap-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://yifsoadlsgen2westus2.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/chaxusqlpool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpoolNew')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_ojz')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [],
				"typeProperties": {
					"schema": "chaxu",
					"table": "newYorkTaxi"
				},
				"sqlPool": {
					"referenceName": "chaxusqlpool",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/chaxusqlpool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_ojz')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "chaxueus2-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "test_data.csv",
						"fileSystem": "chaxufseus"
					},
					"columnDelimiter": ",",
					"rowDelimiter": "\n",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "tipped",
						"type": "String"
					},
					{
						"name": "fareAmount",
						"type": "String"
					},
					{
						"name": "paymentType",
						"type": "String"
					},
					{
						"name": "passengerCount",
						"type": "String"
					},
					{
						"name": "tripDistance",
						"type": "String"
					},
					{
						"name": "tripTimeSecs",
						"type": "String"
					},
					{
						"name": "pickupTimeBin",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/chaxueus2-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVault1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVault1_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspace1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "6c32535b-155f-44ce-83c0-78ca37f16743",
					"tenantID": "72f988bf-86f1-41af-91ab-2d7cd011db47"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bing-covid-19-data')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('bing-covid-19-data_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/chaxuamleus')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('chaxuamleus_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('chaxuamleus_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "chaxuamleus",
					"servicePrincipalId": "[parameters('chaxuamleus_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('chaxuamleus_servicePrincipalKey')]"
					},
					"tenant": "[parameters('chaxuamleus_properties_typeProperties_tenant')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/chaxueus2-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('chaxueus2-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/chaxueus2-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('chaxueus2-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/yifsoeus2euap-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('yifsoeus2euap-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/yifsoeus2euap-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('yifsoeus2euap-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential1')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ServicePrincipal",
				"typeProperties": {
					"tenant": "72f988bf-86f1-41af-91ab-2d7cd011db47",
					"servicePrincipalId": "123",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault1",
							"type": "LinkedServiceReference"
						},
						"secretName": "123"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects WHERE NAME = 'nyc_taxi' AND TYPE = 'U')\nCREATE TABLE dbo.nyc_taxi\n(\n    tipped int,\n    fareAmount float,\n    paymentType int,\n    passengerCount int,\n    tripDistance float,\n    tripTimeSecs bigint,\n    pickupTimeBin nvarchar(30)\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCOPY INTO dbo.nyc_taxi\n(tipped 1, fareAmount 2, paymentType 3, passengerCount 4, tripDistance 5, tripTimeSecs 6, pickupTimeBin 7)\nFROM 'https://chaxustorageeus.dfs.core.windows.net/chaxufseus/test_data.csv'\nWITH\n(\n    FILE_TYPE = 'CSV',\n    ROWTERMINATOR='0x0A',\n    FIELDQUOTE = '\"',\n    FIELDTERMINATOR = ',',\n    FIRSTROW = 2\n)\nGO\n\nSELECT TOP 100 * FROM nyc_taxi\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "chaxusqlpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/loaddata')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects WHERE NAME = 'nyc_taxi_4' AND TYPE = 'U')\nCREATE TABLE dbo.nyc_taxi_4\n(\n\ttipped int,\n\t\"fare.amount\" real,\n\t\"payment.type\" bigint,\n\t\"passenger.count\" int,\n\t\"trip.distance\" float,\n\t\"trip.time.secs\" bigint,\n\t\"pickup.time.bin\" nvarchar(30)\n)\nWITH\n(\n\tDISTRIBUTION = ROUND_ROBIN,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCOPY INTO dbo.nyc_taxi_4\n(tipped 1, \"fare.amount\" 2, \"payment.type\" 3, \"passenger.count\" 4, \"trip.distance\" 5, \"trip.time.secs\" 6, \"pickup.time.bin\" 7)\nFROM 'https://yifsoadlsgen2westus2.dfs.core.windows.net/sparkjob/TestData/test_data.csv'\nWITH\n(\n\tFILE_TYPE = 'CSV',\n\tROWTERMINATOR='0x0A',\n\tFIELDQUOTE = '\"',\n\tFIELDTERMINATOR = ',',\n\tFIRSTROW = 2\n)\nGO\n\nSELECT TOP 100 * FROM nyc_taxi_4\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"poolName": "Built-in",
						"databaseName": "master"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoML_Generated_Notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkpoolMedium",
						"name": "sparkpoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						}
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"nyc_taxi_automl_run2\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.nyc_tlc\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
							"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"regression\",\n",
							"                             training_data = dataset_train,\n",
							"                             label_column_name = \"tipAmount\",\n",
							"                             primary_metric = \"spearman_correlation\",\n",
							"                             experiment_timeout_hours = 1,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = True)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"model = run.register_model(model_name = 'nyc_taxi_automl_run2_best_model', model_path = 'outputs/model.pkl')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoML_Generated_Notebook1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# -------------------------------------------------------------------------------------------------\n",
							"# TODO: Remove this code cell when the latest version of the AML SDK is deployed to the base image.\n",
							"# -------------------------------------------------------------------------------------------------\n",
							"\n",
							"def parse_target(target, add_managed_dataset_prefix=False):\n",
							"    from azureml.data.azure_storage_datastore import AbstractAzureStorageDatastore\n",
							"    from azureml.data.azure_data_lake_datastore import AbstractADLSDatastore\n",
							"    from azureml.data.datapath import DataPath\n",
							"    #from azureml.data.constants import MANAGED_DATASET\n",
							"    MANAGED_DATASET = 'managed-dataset'\n",
							"\n",
							"    datastore = None\n",
							"    relative_path = None\n",
							"\n",
							"    if isinstance(target, AbstractAzureStorageDatastore) or isinstance(target, AbstractADLSDatastore):\n",
							"        datastore = target\n",
							"        relative_path = MANAGED_DATASET if add_managed_dataset_prefix else '/'\n",
							"    elif isinstance(target, DataPath):\n",
							"        datastore = target._datastore\n",
							"        relative_path = (MANAGED_DATASET if add_managed_dataset_prefix else '/') \\\n",
							"            if target.path_on_datastore is None else target.path_on_datastore\n",
							"    elif isinstance(target, tuple) and len(target) == 2:\n",
							"        datastore = target[0]\n",
							"        relative_path = target[1]\n",
							"    if not isinstance(datastore, AbstractAzureStorageDatastore) and not isinstance(datastore, AbstractADLSDatastore):\n",
							"        raise ValueError(\"The target type is not supported, target: {}\".format(target))\n",
							"\n",
							"    return datastore, relative_path\n",
							"\n",
							"\n",
							"def _set_spark_config(datastore):\n",
							"    from pyspark.sql import SparkSession\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"    spark = SparkSession.builder.getOrCreate()\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        account_name = datastore.account_name\n",
							"        account_key = datastore.account_key\n",
							"        endpoint = datastore.endpoint\n",
							"        spark.conf.set('fs.azure.account.key.{}.blob.{}'.format(account_name, endpoint), account_key)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        account_name = datastore.account_name\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        endpoint = datastore.endpoint\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.azure.account\"\n",
							"        provider = \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
							"        storage_account = \"{}.dfs.{}\".format(account_name, endpoint)\n",
							"        spark.conf.set(\"{}.auth.type.{}\".format(prefix, storage_account), \"OAuth\")\n",
							"        spark.conf.set(\"{}.oauth.provider.type.{}\".format(prefix, storage_account), provider)\n",
							"        spark.conf.set(\"{}.oauth2.client.id.{}\".format(prefix, storage_account), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.client.secret.{}\".format(prefix, storage_account), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.client.endpoint.{}\".format(prefix, storage_account),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.adl\"  # dfs.adls deprecated\n",
							"        spark.conf.set(\"{}.oauth2.access.token.provider.type\".format(prefix), \"ClientCredential\")\n",
							"        spark.conf.set(\"{}.oauth2.client.id\".format(prefix), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.credential\".format(prefix), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.refresh.url\".format(prefix),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"\n",
							"def _get_output_uri(datastore, path):\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        output_uri = 'wasbs://{}@{}.blob.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                       datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        output_uri = 'abfss://{}@{}.dfs.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                      datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        output_uri = 'adl://{}.azuredatalakestore.net/{}'.format(datastore.store_name, path)\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"    return output_uri\n",
							"\n",
							"\n",
							"def write_spark_dataframe(spark_dataframe, datastore, relative_path_with_guid, show_progress):\n",
							"    console = get_progress_logger(show_progress)\n",
							"    _set_spark_config(datastore)\n",
							"    output_uri = _get_output_uri(datastore, relative_path_with_guid)\n",
							"\n",
							"    console(\"Writing spark dataframe to {}\".format(relative_path_with_guid))\n",
							"    spark_dataframe.write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save(output_uri)\n",
							"\n",
							"\n",
							"def get_progress_logger(show_progress):\n",
							"    import sys\n",
							"    console = sys.stdout\n",
							"\n",
							"    def log(message):\n",
							"        if show_progress:\n",
							"            console.write(\"{}\\n\".format(message))\n",
							"\n",
							"    return log\n",
							"\n",
							"def _check_type(arg, arg_name, expected_type):\n",
							"    if not isinstance(arg, expected_type):\n",
							"        raise ValueError(\"Expected {} of type {} but received {}\".format(arg_name, expected_type, type(arg)))\n",
							"\n",
							"def register_spark_dataframe(dataframe, target, name, description=None, tags=None, show_progress=True):\n",
							"        \"\"\"Create a dataset from spark dataframe.\n",
							"\n",
							"        :param dataframe: Required, in memory dataframe to be uploaded.\n",
							"        :type dataframe: pyspark.sql.DataFrame\n",
							"        :param target: Required, the datastore path where the dataframe parquet data will be uploaded to.\n",
							"            A guid folder will be generated under the target path to avoid conflict.\n",
							"        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
							"            or tuple(azureml.core.datastore.Datastore, str) object\n",
							"        :param name: Required, the name of the registered dataset.\n",
							"        :type name: str\n",
							"        :param description: Optional. A text description of the dataset. Defaults to None.\n",
							"        :type description: str\n",
							"        :param tags: Optional. Dictionary of key value tags to give the dataset. Defaults to None.\n",
							"        :type tags: dict[str, str]\n",
							"        :param show_progress: Optional, indicates whether to show progress of the upload in the console.\n",
							"            Defaults to be True.\n",
							"        :type show_progress: bool\n",
							"        :return: The registered dataset.\n",
							"        :rtype: azureml.data.TabularDataset\n",
							"        \"\"\"\n",
							"        from azureml.data.datapath import DataPath\n",
							"        from pyspark.sql import DataFrame\n",
							"        from uuid import uuid4\n",
							"\n",
							"        console = get_progress_logger(show_progress)\n",
							"\n",
							"        console(\"Validating arguments.\")\n",
							"        _check_type(dataframe, \"dataframe\", DataFrame)\n",
							"        _check_type(name, \"name\", str)\n",
							"        datastore, relative_path = parse_target(target, True)\n",
							"        console(\"Arguments validated.\")\n",
							"\n",
							"        guid = uuid4()\n",
							"        relative_path_with_guid = \"{}/{}\".format(relative_path, guid)\n",
							"        write_spark_dataframe(dataframe, datastore, relative_path_with_guid, show_progress)\n",
							"\n",
							"        console(\"Creating new dataset\")\n",
							"        datapath = DataPath(datastore, \"/{}/*.parquet\".format(relative_path_with_guid))\n",
							"        saved_dataset = Dataset.Tabular.from_parquet_files(datapath)\n",
							"\n",
							"        console(\"Registering new dataset\")\n",
							"        registered_dataset = saved_dataset.register(datastore.workspace, name,\n",
							"                                                    description=description,\n",
							"                                                    tags=tags,\n",
							"                                                    create_new_version=True)\n",
							"        console(\"Successfully created and registered a new dataset.\")\n",
							"\n",
							"        return registered_dataset\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"nyc_taxi_automl_run2\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.nyc_tlc\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
							"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"regression\",\n",
							"                             training_data = dataset_train,\n",
							"                             label_column_name = \"tipAmount\",\n",
							"                             primary_metric = \"spearman_correlation\",\n",
							"                             experiment_timeout_hours = 1,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"model = run.register_model(model_name = 'nyc_taxi_automl_run2_best_model', model_path = 'outputs/model.pkl')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoML_NYC_taxi')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkpoolMedium",
						"name": "sparkpoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# -------------------------------------------------------------------------------------------------\n",
							"# TODO: Remove this code cell when the latest version of the AML SDK is deployed to the base image.\n",
							"# -------------------------------------------------------------------------------------------------\n",
							"\n",
							"def parse_target(target, add_managed_dataset_prefix=False):\n",
							"    from azureml.data.azure_storage_datastore import AbstractAzureStorageDatastore\n",
							"    from azureml.data.azure_data_lake_datastore import AbstractADLSDatastore\n",
							"    from azureml.data.datapath import DataPath\n",
							"    #from azureml.data.constants import MANAGED_DATASET\n",
							"    MANAGED_DATASET = 'managed-dataset'\n",
							"\n",
							"    datastore = None\n",
							"    relative_path = None\n",
							"\n",
							"    if isinstance(target, AbstractAzureStorageDatastore) or isinstance(target, AbstractADLSDatastore):\n",
							"        datastore = target\n",
							"        relative_path = MANAGED_DATASET if add_managed_dataset_prefix else '/'\n",
							"    elif isinstance(target, DataPath):\n",
							"        datastore = target._datastore\n",
							"        relative_path = (MANAGED_DATASET if add_managed_dataset_prefix else '/') \\\n",
							"            if target.path_on_datastore is None else target.path_on_datastore\n",
							"    elif isinstance(target, tuple) and len(target) == 2:\n",
							"        datastore = target[0]\n",
							"        relative_path = target[1]\n",
							"    if not isinstance(datastore, AbstractAzureStorageDatastore) and not isinstance(datastore, AbstractADLSDatastore):\n",
							"        raise ValueError(\"The target type is not supported, target: {}\".format(target))\n",
							"\n",
							"    return datastore, relative_path\n",
							"\n",
							"\n",
							"def _set_spark_config(datastore):\n",
							"    from pyspark.sql import SparkSession\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"    spark = SparkSession.builder.getOrCreate()\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        account_name = datastore.account_name\n",
							"        account_key = datastore.account_key\n",
							"        endpoint = datastore.endpoint\n",
							"        spark.conf.set('fs.azure.account.key.{}.blob.{}'.format(account_name, endpoint), account_key)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        account_name = datastore.account_name\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        endpoint = datastore.endpoint\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.azure.account\"\n",
							"        provider = \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
							"        storage_account = \"{}.dfs.{}\".format(account_name, endpoint)\n",
							"        spark.conf.set(\"{}.auth.type.{}\".format(prefix, storage_account), \"OAuth\")\n",
							"        spark.conf.set(\"{}.oauth.provider.type.{}\".format(prefix, storage_account), provider)\n",
							"        spark.conf.set(\"{}.oauth2.client.id.{}\".format(prefix, storage_account), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.client.secret.{}\".format(prefix, storage_account), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.client.endpoint.{}\".format(prefix, storage_account),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.adl\"  # dfs.adls deprecated\n",
							"        spark.conf.set(\"{}.oauth2.access.token.provider.type\".format(prefix), \"ClientCredential\")\n",
							"        spark.conf.set(\"{}.oauth2.client.id\".format(prefix), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.credential\".format(prefix), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.refresh.url\".format(prefix),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"\n",
							"def _get_output_uri(datastore, path):\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        output_uri = 'wasbs://{}@{}.blob.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                       datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        output_uri = 'abfss://{}@{}.dfs.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                      datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        output_uri = 'adl://{}.azuredatalakestore.net/{}'.format(datastore.store_name, path)\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"    return output_uri\n",
							"\n",
							"\n",
							"def write_spark_dataframe(spark_dataframe, datastore, relative_path_with_guid, show_progress):\n",
							"    console = get_progress_logger(show_progress)\n",
							"    _set_spark_config(datastore)\n",
							"    output_uri = _get_output_uri(datastore, relative_path_with_guid)\n",
							"\n",
							"    console(\"Writing spark dataframe to {}\".format(relative_path_with_guid))\n",
							"    spark_dataframe.write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save(output_uri)\n",
							"\n",
							"\n",
							"def get_progress_logger(show_progress):\n",
							"    import sys\n",
							"    console = sys.stdout\n",
							"\n",
							"    def log(message):\n",
							"        if show_progress:\n",
							"            console.write(\"{}\\n\".format(message))\n",
							"\n",
							"    return log\n",
							"\n",
							"def _check_type(arg, arg_name, expected_type):\n",
							"    if not isinstance(arg, expected_type):\n",
							"        raise ValueError(\"Expected {} of type {} but received {}\".format(arg_name, expected_type, type(arg)))\n",
							"\n",
							"def register_spark_dataframe(dataframe, target, name, description=None, tags=None, show_progress=True):\n",
							"        \"\"\"Create a dataset from spark dataframe.\n",
							"\n",
							"        :param dataframe: Required, in memory dataframe to be uploaded.\n",
							"        :type dataframe: pyspark.sql.DataFrame\n",
							"        :param target: Required, the datastore path where the dataframe parquet data will be uploaded to.\n",
							"            A guid folder will be generated under the target path to avoid conflict.\n",
							"        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
							"            or tuple(azureml.core.datastore.Datastore, str) object\n",
							"        :param name: Required, the name of the registered dataset.\n",
							"        :type name: str\n",
							"        :param description: Optional. A text description of the dataset. Defaults to None.\n",
							"        :type description: str\n",
							"        :param tags: Optional. Dictionary of key value tags to give the dataset. Defaults to None.\n",
							"        :type tags: dict[str, str]\n",
							"        :param show_progress: Optional, indicates whether to show progress of the upload in the console.\n",
							"            Defaults to be True.\n",
							"        :type show_progress: bool\n",
							"        :return: The registered dataset.\n",
							"        :rtype: azureml.data.TabularDataset\n",
							"        \"\"\"\n",
							"        from azureml.data.datapath import DataPath\n",
							"        from pyspark.sql import DataFrame\n",
							"        from uuid import uuid4\n",
							"\n",
							"        console = get_progress_logger(show_progress)\n",
							"\n",
							"        console(\"Validating arguments.\")\n",
							"        _check_type(dataframe, \"dataframe\", DataFrame)\n",
							"        _check_type(name, \"name\", str)\n",
							"        datastore, relative_path = parse_target(target, True)\n",
							"        console(\"Arguments validated.\")\n",
							"\n",
							"        guid = uuid4()\n",
							"        relative_path_with_guid = \"{}/{}\".format(relative_path, guid)\n",
							"        write_spark_dataframe(dataframe, datastore, relative_path_with_guid, show_progress)\n",
							"\n",
							"        console(\"Creating new dataset\")\n",
							"        datapath = DataPath(datastore, \"/{}/*.parquet\".format(relative_path_with_guid))\n",
							"        saved_dataset = Dataset.Tabular.from_parquet_files(datapath)\n",
							"\n",
							"        console(\"Registering new dataset\")\n",
							"        registered_dataset = saved_dataset.register(datastore.workspace, name,\n",
							"                                                    description=description,\n",
							"                                                    tags=tags,\n",
							"                                                    create_new_version=True)\n",
							"        console(\"Successfully created and registered a new dataset.\")\n",
							"\n",
							"        return registered_dataset\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"nyc_taxi_automl_run1\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.nyc_tlc\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
							"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"classification\",\n",
							"                             training_data = dataset_train,\n",
							"                             label_column_name = \"paymentType\",\n",
							"                             primary_metric = \"accuracy\",\n",
							"                             experiment_timeout_hours = 3,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"model = run.register_model(model_name = 'best_model', model_path = 'outputs/model.pkl')\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoML_Notebook_Executed')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkpoolMedium",
						"name": "sparkpoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# -------------------------------------------------------------------------------------------------\n",
							"# TODO: Remove this code cell when the latest version of the AML SDK is deployed to the base image.\n",
							"# -------------------------------------------------------------------------------------------------\n",
							"\n",
							"def parse_target(target, add_managed_dataset_prefix=False):\n",
							"    from azureml.data.azure_storage_datastore import AbstractAzureStorageDatastore\n",
							"    from azureml.data.azure_data_lake_datastore import AbstractADLSDatastore\n",
							"    from azureml.data.datapath import DataPath\n",
							"    #from azureml.data.constants import MANAGED_DATASET\n",
							"    MANAGED_DATASET = 'managed-dataset'\n",
							"\n",
							"    datastore = None\n",
							"    relative_path = None\n",
							"\n",
							"    if isinstance(target, AbstractAzureStorageDatastore) or isinstance(target, AbstractADLSDatastore):\n",
							"        datastore = target\n",
							"        relative_path = MANAGED_DATASET if add_managed_dataset_prefix else '/'\n",
							"    elif isinstance(target, DataPath):\n",
							"        datastore = target._datastore\n",
							"        relative_path = (MANAGED_DATASET if add_managed_dataset_prefix else '/') \\\n",
							"            if target.path_on_datastore is None else target.path_on_datastore\n",
							"    elif isinstance(target, tuple) and len(target) == 2:\n",
							"        datastore = target[0]\n",
							"        relative_path = target[1]\n",
							"    if not isinstance(datastore, AbstractAzureStorageDatastore) and not isinstance(datastore, AbstractADLSDatastore):\n",
							"        raise ValueError(\"The target type is not supported, target: {}\".format(target))\n",
							"\n",
							"    return datastore, relative_path\n",
							"\n",
							"\n",
							"def _set_spark_config(datastore):\n",
							"    from pyspark.sql import SparkSession\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"    spark = SparkSession.builder.getOrCreate()\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        account_name = datastore.account_name\n",
							"        account_key = datastore.account_key\n",
							"        endpoint = datastore.endpoint\n",
							"        spark.conf.set('fs.azure.account.key.{}.blob.{}'.format(account_name, endpoint), account_key)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        account_name = datastore.account_name\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        endpoint = datastore.endpoint\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.azure.account\"\n",
							"        provider = \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
							"        storage_account = \"{}.dfs.{}\".format(account_name, endpoint)\n",
							"        spark.conf.set(\"{}.auth.type.{}\".format(prefix, storage_account), \"OAuth\")\n",
							"        spark.conf.set(\"{}.oauth.provider.type.{}\".format(prefix, storage_account), provider)\n",
							"        spark.conf.set(\"{}.oauth2.client.id.{}\".format(prefix, storage_account), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.client.secret.{}\".format(prefix, storage_account), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.client.endpoint.{}\".format(prefix, storage_account),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.adl\"  # dfs.adls deprecated\n",
							"        spark.conf.set(\"{}.oauth2.access.token.provider.type\".format(prefix), \"ClientCredential\")\n",
							"        spark.conf.set(\"{}.oauth2.client.id\".format(prefix), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.credential\".format(prefix), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.refresh.url\".format(prefix),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"\n",
							"def _get_output_uri(datastore, path):\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        output_uri = 'wasbs://{}@{}.blob.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                       datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        output_uri = 'abfss://{}@{}.dfs.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                      datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        output_uri = 'adl://{}.azuredatalakestore.net/{}'.format(datastore.store_name, path)\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"    return output_uri\n",
							"\n",
							"\n",
							"def write_spark_dataframe(spark_dataframe, datastore, relative_path_with_guid, show_progress):\n",
							"    console = get_progress_logger(show_progress)\n",
							"    _set_spark_config(datastore)\n",
							"    output_uri = _get_output_uri(datastore, relative_path_with_guid)\n",
							"\n",
							"    console(\"Writing spark dataframe to {}\".format(relative_path_with_guid))\n",
							"    spark_dataframe.write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save(output_uri)\n",
							"\n",
							"\n",
							"def get_progress_logger(show_progress):\n",
							"    import sys\n",
							"    console = sys.stdout\n",
							"\n",
							"    def log(message):\n",
							"        if show_progress:\n",
							"            console.write(\"{}\\n\".format(message))\n",
							"\n",
							"    return log\n",
							"\n",
							"def _check_type(arg, arg_name, expected_type):\n",
							"    if not isinstance(arg, expected_type):\n",
							"        raise ValueError(\"Expected {} of type {} but received {}\".format(arg_name, expected_type, type(arg)))\n",
							"\n",
							"def register_spark_dataframe(dataframe, target, name, description=None, tags=None, show_progress=True):\n",
							"        \"\"\"Create a dataset from spark dataframe.\n",
							"\n",
							"        :param dataframe: Required, in memory dataframe to be uploaded.\n",
							"        :type dataframe: pyspark.sql.DataFrame\n",
							"        :param target: Required, the datastore path where the dataframe parquet data will be uploaded to.\n",
							"            A guid folder will be generated under the target path to avoid conflict.\n",
							"        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
							"            or tuple(azureml.core.datastore.Datastore, str) object\n",
							"        :param name: Required, the name of the registered dataset.\n",
							"        :type name: str\n",
							"        :param description: Optional. A text description of the dataset. Defaults to None.\n",
							"        :type description: str\n",
							"        :param tags: Optional. Dictionary of key value tags to give the dataset. Defaults to None.\n",
							"        :type tags: dict[str, str]\n",
							"        :param show_progress: Optional, indicates whether to show progress of the upload in the console.\n",
							"            Defaults to be True.\n",
							"        :type show_progress: bool\n",
							"        :return: The registered dataset.\n",
							"        :rtype: azureml.data.TabularDataset\n",
							"        \"\"\"\n",
							"        from azureml.data.datapath import DataPath\n",
							"        from pyspark.sql import DataFrame\n",
							"        from uuid import uuid4\n",
							"\n",
							"        console = get_progress_logger(show_progress)\n",
							"\n",
							"        console(\"Validating arguments.\")\n",
							"        _check_type(dataframe, \"dataframe\", DataFrame)\n",
							"        _check_type(name, \"name\", str)\n",
							"        datastore, relative_path = parse_target(target, True)\n",
							"        console(\"Arguments validated.\")\n",
							"\n",
							"        guid = uuid4()\n",
							"        relative_path_with_guid = \"{}/{}\".format(relative_path, guid)\n",
							"        write_spark_dataframe(dataframe, datastore, relative_path_with_guid, show_progress)\n",
							"\n",
							"        console(\"Creating new dataset\")\n",
							"        datapath = DataPath(datastore, \"/{}/*.parquet\".format(relative_path_with_guid))\n",
							"        saved_dataset = Dataset.Tabular.from_parquet_files(datapath)\n",
							"\n",
							"        console(\"Registering new dataset\")\n",
							"        registered_dataset = saved_dataset.register(datastore.workspace, name,\n",
							"                                                    description=description,\n",
							"                                                    tags=tags,\n",
							"                                                    create_new_version=True)\n",
							"        console(\"Successfully created and registered a new dataset.\")\n",
							"\n",
							"        return registered_dataset\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"12345\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.nyc_tlc\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
							"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"regression\",\n",
							"                             training_data = dataset_train,\n",
							"                             label_column_name = \"fareAmount\",\n",
							"                             primary_metric = \"spearman_correlation\",\n",
							"                             experiment_timeout_hours = 1,\n",
							"                             max_concurrent_iterations = 50,\n",
							"                             enable_onnx_compatible_models = True)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# model = run.register_model(model_name = '12345_best_model', model_path = 'outputs/model.pkl')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzML_import_test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool2",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/yifso-workspace/providers/Microsoft.Synapse/workspaces/yifsoeus2euap/bigDataPools/sparkpool2",
						"name": "sparkpool2",
						"type": "Spark",
						"endpoint": "https://yifsoeus2euap.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool2",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\r\n",
							"azureml.core.__version__"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import azureml.opendataset"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import azure.storage.blob\r\n",
							"azure.storage.blob.__version__"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import azureml.mlflow\r\n",
							"azureml.mlflow.__version__"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CogSvc_TextSentiment')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/yifso-workspace/providers/Microsoft.Synapse/workspaces/yifsoeus2euap/bigDataPools/sparkpool1",
						"name": "sparkpool1",
						"type": "Spark",
						"endpoint": "https://yifsoeus2euap.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from mmlspark.cognitive import *\n",
							"\n",
							"# Fetch the subscription key (or a general Cognitive Service key) from Azure Key Vault\n",
							"service_key = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary.getAkvSecret(\"AzureKeyVault1\", \"yifso-akv\", \"yifso-text-analytics-secret\")\n",
							"\n",
							"# Load the data into a Spark DataFrame\n",
							"df = spark.sql(\"SELECT * FROM default.textsentimentdata\")\n",
							"\n",
							"sentiment = (TextSentiment()\n",
							"    .setLocation(\"westus2\")\n",
							"    .setSubscriptionKey(service_key)\n",
							"    .setOutputCol(\"output\")\n",
							"    .setErrorCol(\"error\")\n",
							"    .setLanguage(\"en\")\n",
							"    .setTextCol(\"text\"))\n",
							"\n",
							"results = sentiment.transform(df)\n",
							"\n",
							"# Show the results\n",
							"display(results.select(\"text\", \"output\", \"error\").limit(5))\n",
							"\n",
							"results.write.mode(\"overwrite\").saveAsTable(\"TextSentimentResult\")"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create spark db')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkpoolMedium",
						"name": "sparkpoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"spark.Sql(\"CREATE DATABASE mytestdb\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create spark table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkpoolMedium",
						"name": "sparkpoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE TABLE mytestdb.myParquetTable(myId int, myName string, myBirthdate date) USING Parquet"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get Akv secret API')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/yifso-workspace/providers/Microsoft.Synapse/workspaces/yifsoeus2euap/bigDataPools/sparkpool1",
						"name": "sparkpool1",
						"type": "Spark",
						"endpoint": "https://yifsoeus2euap.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Python API\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Python API (1)\n",
							"# Unfortunately, it doesn't work currently. Long's team is working on a fix.\n",
							"\n",
							"from notebookutils.mssparkutils import credentials \n",
							"\n",
							"secret = credentials.getSecret(\"AzureKeyVault1\", \"yifso-akv\", \"mysecret1\")\n",
							"print(secret)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Python API (2)\n",
							"\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"secret = token_library.getAkvSecret(\"AzureKeyVault1\", \"yifso-akv\", \"mysecret1\")\n",
							"print(secret)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Scala API\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%spark\n",
							"val secret = com.microsoft.azure.synapse.tokenlibrary.TokenLibrary.getAkvSecret(\"AzureKeyVault1\", \"yifso-akv\", \"mysecret1\")"
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkpoolMedium",
						"name": "sparkpoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# -------------------------------------------------------------------------------------------------\n",
							"# TODO: Remove this code cell when the latest version of the AML SDK is deployed to the base image.\n",
							"# -------------------------------------------------------------------------------------------------\n",
							"\n",
							"def parse_target(target, add_managed_dataset_prefix=False):\n",
							"    from azureml.data.azure_storage_datastore import AbstractAzureStorageDatastore\n",
							"    from azureml.data.azure_data_lake_datastore import AbstractADLSDatastore\n",
							"    from azureml.data.datapath import DataPath\n",
							"    #from azureml.data.constants import MANAGED_DATASET\n",
							"    MANAGED_DATASET = 'managed-dataset'\n",
							"\n",
							"    datastore = None\n",
							"    relative_path = None\n",
							"\n",
							"    if isinstance(target, AbstractAzureStorageDatastore) or isinstance(target, AbstractADLSDatastore):\n",
							"        datastore = target\n",
							"        relative_path = MANAGED_DATASET if add_managed_dataset_prefix else '/'\n",
							"    elif isinstance(target, DataPath):\n",
							"        datastore = target._datastore\n",
							"        relative_path = (MANAGED_DATASET if add_managed_dataset_prefix else '/') \\\n",
							"            if target.path_on_datastore is None else target.path_on_datastore\n",
							"    elif isinstance(target, tuple) and len(target) == 2:\n",
							"        datastore = target[0]\n",
							"        relative_path = target[1]\n",
							"    if not isinstance(datastore, AbstractAzureStorageDatastore) and not isinstance(datastore, AbstractADLSDatastore):\n",
							"        raise ValueError(\"The target type is not supported, target: {}\".format(target))\n",
							"\n",
							"    return datastore, relative_path\n",
							"\n",
							"\n",
							"def _set_spark_config(datastore):\n",
							"    from pyspark.sql import SparkSession\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"    spark = SparkSession.builder.getOrCreate()\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        account_name = datastore.account_name\n",
							"        account_key = datastore.account_key\n",
							"        endpoint = datastore.endpoint\n",
							"        spark.conf.set('fs.azure.account.key.{}.blob.{}'.format(account_name, endpoint), account_key)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        account_name = datastore.account_name\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        endpoint = datastore.endpoint\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.azure.account\"\n",
							"        provider = \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
							"        storage_account = \"{}.dfs.{}\".format(account_name, endpoint)\n",
							"        spark.conf.set(\"{}.auth.type.{}\".format(prefix, storage_account), \"OAuth\")\n",
							"        spark.conf.set(\"{}.oauth.provider.type.{}\".format(prefix, storage_account), provider)\n",
							"        spark.conf.set(\"{}.oauth2.client.id.{}\".format(prefix, storage_account), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.client.secret.{}\".format(prefix, storage_account), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.client.endpoint.{}\".format(prefix, storage_account),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.adl\"  # dfs.adls deprecated\n",
							"        spark.conf.set(\"{}.oauth2.access.token.provider.type\".format(prefix), \"ClientCredential\")\n",
							"        spark.conf.set(\"{}.oauth2.client.id\".format(prefix), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.credential\".format(prefix), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.refresh.url\".format(prefix),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"\n",
							"def _get_output_uri(datastore, path):\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        output_uri = 'wasbs://{}@{}.blob.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                       datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        output_uri = 'abfss://{}@{}.dfs.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                      datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        output_uri = 'adl://{}.azuredatalakestore.net/{}'.format(datastore.store_name, path)\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"    return output_uri\n",
							"\n",
							"\n",
							"def write_spark_dataframe(spark_dataframe, datastore, relative_path_with_guid, show_progress):\n",
							"    console = get_progress_logger(show_progress)\n",
							"    _set_spark_config(datastore)\n",
							"    output_uri = _get_output_uri(datastore, relative_path_with_guid)\n",
							"\n",
							"    console(\"Writing spark dataframe to {}\".format(relative_path_with_guid))\n",
							"    spark_dataframe.write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save(output_uri)\n",
							"\n",
							"\n",
							"def get_progress_logger(show_progress):\n",
							"    import sys\n",
							"    console = sys.stdout\n",
							"\n",
							"    def log(message):\n",
							"        if show_progress:\n",
							"            console.write(\"{}\\n\".format(message))\n",
							"\n",
							"    return log\n",
							"\n",
							"def _check_type(arg, arg_name, expected_type):\n",
							"    if not isinstance(arg, expected_type):\n",
							"        raise ValueError(\"Expected {} of type {} but received {}\".format(arg_name, expected_type, type(arg)))\n",
							"\n",
							"def register_spark_dataframe(dataframe, target, name, description=None, tags=None, show_progress=True):\n",
							"        \"\"\"Create a dataset from spark dataframe.\n",
							"\n",
							"        :param dataframe: Required, in memory dataframe to be uploaded.\n",
							"        :type dataframe: pyspark.sql.DataFrame\n",
							"        :param target: Required, the datastore path where the dataframe parquet data will be uploaded to.\n",
							"            A guid folder will be generated under the target path to avoid conflict.\n",
							"        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
							"            or tuple(azureml.core.datastore.Datastore, str) object\n",
							"        :param name: Required, the name of the registered dataset.\n",
							"        :type name: str\n",
							"        :param description: Optional. A text description of the dataset. Defaults to None.\n",
							"        :type description: str\n",
							"        :param tags: Optional. Dictionary of key value tags to give the dataset. Defaults to None.\n",
							"        :type tags: dict[str, str]\n",
							"        :param show_progress: Optional, indicates whether to show progress of the upload in the console.\n",
							"            Defaults to be True.\n",
							"        :type show_progress: bool\n",
							"        :return: The registered dataset.\n",
							"        :rtype: azureml.data.TabularDataset\n",
							"        \"\"\"\n",
							"        from azureml.data.datapath import DataPath\n",
							"        from pyspark.sql import DataFrame\n",
							"        from uuid import uuid4\n",
							"\n",
							"        console = get_progress_logger(show_progress)\n",
							"\n",
							"        console(\"Validating arguments.\")\n",
							"        _check_type(dataframe, \"dataframe\", DataFrame)\n",
							"        _check_type(name, \"name\", str)\n",
							"        datastore, relative_path = parse_target(target, True)\n",
							"        console(\"Arguments validated.\")\n",
							"\n",
							"        guid = uuid4()\n",
							"        relative_path_with_guid = \"{}/{}\".format(relative_path, guid)\n",
							"        write_spark_dataframe(dataframe, datastore, relative_path_with_guid, show_progress)\n",
							"\n",
							"        console(\"Creating new dataset\")\n",
							"        datapath = DataPath(datastore, \"/{}/*.parquet\".format(relative_path_with_guid))\n",
							"        saved_dataset = Dataset.Tabular.from_parquet_files(datapath)\n",
							"\n",
							"        console(\"Registering new dataset\")\n",
							"        registered_dataset = saved_dataset.register(datastore.workspace, name,\n",
							"                                                    description=description,\n",
							"                                                    tags=tags,\n",
							"                                                    create_new_version=True)\n",
							"        console(\"Successfully created and registered a new dataset.\")\n",
							"\n",
							"        return registered_dataset\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"nyc_taxi_automl_run1\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM mytestdb.nyc_taxi\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
							"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"classification\",\n",
							"                             training_data = dataset_train,\n",
							"                             label_column_name = \"tipped\",\n",
							"                             primary_metric = \"accuracy\",\n",
							"                             experiment_timeout_hours = 3,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"model = run.register_model(model_name = 'best_model', model_path = 'outputs/model.pkl')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkPoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkPoolMedium",
						"name": "sparkPoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkPoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"try:\r\n",
							"    from azureml._base_sdk_common.user_agent import append\r\n",
							"    append(\"SynapseSparkPool\", product_version=\"1.0.5\")    \r\n",
							"except: # Always swallow exceptions\r\n",
							"    print('Set user agent failed')         "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from azureml._base_sdk_common.user_agent import user_agent_product_list \r\n",
							"for ua in user_agent_product_list:\r\n",
							"    print(ua.name)\r\n",
							"    print(ua.version)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkPoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkPoolMedium",
						"name": "sparkPoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkPoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"inputCollapsed": true
						},
						"source": [
							"# -------------------------------------------------------------------------------------------------\n",
							"# TODO: Remove this code cell when the latest version of the AML SDK is deployed to the base image.\n",
							"# -------------------------------------------------------------------------------------------------\n",
							"\n",
							"def parse_target(target, add_managed_dataset_prefix=False):\n",
							"    from azureml.data.azure_storage_datastore import AbstractAzureStorageDatastore\n",
							"    from azureml.data.azure_data_lake_datastore import AbstractADLSDatastore\n",
							"    from azureml.data.datapath import DataPath\n",
							"    #from azureml.data.constants import MANAGED_DATASET\n",
							"    MANAGED_DATASET = 'managed-dataset'\n",
							"\n",
							"    datastore = None\n",
							"    relative_path = None\n",
							"\n",
							"    if isinstance(target, AbstractAzureStorageDatastore) or isinstance(target, AbstractADLSDatastore):\n",
							"        datastore = target\n",
							"        relative_path = MANAGED_DATASET if add_managed_dataset_prefix else '/'\n",
							"    elif isinstance(target, DataPath):\n",
							"        datastore = target._datastore\n",
							"        relative_path = (MANAGED_DATASET if add_managed_dataset_prefix else '/') \\\n",
							"            if target.path_on_datastore is None else target.path_on_datastore\n",
							"    elif isinstance(target, tuple) and len(target) == 2:\n",
							"        datastore = target[0]\n",
							"        relative_path = target[1]\n",
							"    if not isinstance(datastore, AbstractAzureStorageDatastore) and not isinstance(datastore, AbstractADLSDatastore):\n",
							"        raise ValueError(\"The target type is not supported, target: {}\".format(target))\n",
							"\n",
							"    return datastore, relative_path\n",
							"\n",
							"\n",
							"def _set_spark_config(datastore):\n",
							"    from pyspark.sql import SparkSession\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"    spark = SparkSession.builder.getOrCreate()\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        account_name = datastore.account_name\n",
							"        account_key = datastore.account_key\n",
							"        endpoint = datastore.endpoint\n",
							"        spark.conf.set('fs.azure.account.key.{}.blob.{}'.format(account_name, endpoint), account_key)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        account_name = datastore.account_name\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        endpoint = datastore.endpoint\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.azure.account\"\n",
							"        provider = \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
							"        storage_account = \"{}.dfs.{}\".format(account_name, endpoint)\n",
							"        spark.conf.set(\"{}.auth.type.{}\".format(prefix, storage_account), \"OAuth\")\n",
							"        spark.conf.set(\"{}.oauth.provider.type.{}\".format(prefix, storage_account), provider)\n",
							"        spark.conf.set(\"{}.oauth2.client.id.{}\".format(prefix, storage_account), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.client.secret.{}\".format(prefix, storage_account), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.client.endpoint.{}\".format(prefix, storage_account),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.adl\"  # dfs.adls deprecated\n",
							"        spark.conf.set(\"{}.oauth2.access.token.provider.type\".format(prefix), \"ClientCredential\")\n",
							"        spark.conf.set(\"{}.oauth2.client.id\".format(prefix), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.credential\".format(prefix), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.refresh.url\".format(prefix),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"\n",
							"def _get_output_uri(datastore, path):\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        output_uri = 'wasbs://{}@{}.blob.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                       datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        output_uri = 'abfss://{}@{}.dfs.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                      datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        output_uri = 'adl://{}.azuredatalakestore.net/{}'.format(datastore.store_name, path)\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"    return output_uri\n",
							"\n",
							"\n",
							"def write_spark_dataframe(spark_dataframe, datastore, relative_path_with_guid, show_progress):\n",
							"    console = get_progress_logger(show_progress)\n",
							"    _set_spark_config(datastore)\n",
							"    output_uri = _get_output_uri(datastore, relative_path_with_guid)\n",
							"\n",
							"    console(\"Writing spark dataframe to {}\".format(relative_path_with_guid))\n",
							"    spark_dataframe.write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save(output_uri)\n",
							"\n",
							"\n",
							"def get_progress_logger(show_progress):\n",
							"    import sys\n",
							"    console = sys.stdout\n",
							"\n",
							"    def log(message):\n",
							"        if show_progress:\n",
							"            console.write(\"{}\\n\".format(message))\n",
							"\n",
							"    return log\n",
							"\n",
							"def _check_type(arg, arg_name, expected_type):\n",
							"    if not isinstance(arg, expected_type):\n",
							"        raise ValueError(\"Expected {} of type {} but received {}\".format(arg_name, expected_type, type(arg)))\n",
							"\n",
							"def register_spark_dataframe(dataframe, target, name, description=None, tags=None, show_progress=True):\n",
							"        \"\"\"Create a dataset from spark dataframe.\n",
							"\n",
							"        :param dataframe: Required, in memory dataframe to be uploaded.\n",
							"        :type dataframe: pyspark.sql.DataFrame\n",
							"        :param target: Required, the datastore path where the dataframe parquet data will be uploaded to.\n",
							"            A guid folder will be generated under the target path to avoid conflict.\n",
							"        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
							"            or tuple(azureml.core.datastore.Datastore, str) object\n",
							"        :param name: Required, the name of the registered dataset.\n",
							"        :type name: str\n",
							"        :param description: Optional. A text description of the dataset. Defaults to None.\n",
							"        :type description: str\n",
							"        :param tags: Optional. Dictionary of key value tags to give the dataset. Defaults to None.\n",
							"        :type tags: dict[str, str]\n",
							"        :param show_progress: Optional, indicates whether to show progress of the upload in the console.\n",
							"            Defaults to be True.\n",
							"        :type show_progress: bool\n",
							"        :return: The registered dataset.\n",
							"        :rtype: azureml.data.TabularDataset\n",
							"        \"\"\"\n",
							"        from azureml.data.datapath import DataPath\n",
							"        from pyspark.sql import DataFrame\n",
							"        from uuid import uuid4\n",
							"\n",
							"        console = get_progress_logger(show_progress)\n",
							"\n",
							"        console(\"Validating arguments.\")\n",
							"        _check_type(dataframe, \"dataframe\", DataFrame)\n",
							"        _check_type(name, \"name\", str)\n",
							"        datastore, relative_path = parse_target(target, True)\n",
							"        console(\"Arguments validated.\")\n",
							"\n",
							"        guid = uuid4()\n",
							"        relative_path_with_guid = \"{}/{}\".format(relative_path, guid)\n",
							"        write_spark_dataframe(dataframe, datastore, relative_path_with_guid, show_progress)\n",
							"\n",
							"        console(\"Creating new dataset\")\n",
							"        datapath = DataPath(datastore, \"/{}/*.parquet\".format(relative_path_with_guid))\n",
							"        saved_dataset = Dataset.Tabular.from_parquet_files(datapath)\n",
							"\n",
							"        console(\"Registering new dataset\")\n",
							"        registered_dataset = saved_dataset.register(datastore.workspace, name,\n",
							"                                                    description=description,\n",
							"                                                    tags=tags,\n",
							"                                                    create_new_version=True)\n",
							"        console(\"Successfully created and registered a new dataset.\")\n",
							"\n",
							"        return registered_dataset\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"AutoML-WithRegistration-20201101071229\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.nyc_taxi\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
							"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"classification\",\n",
							"                             training_data = dataset_train,\n",
							"                             label_column_name = \"tipped\",\n",
							"                             primary_metric = \"accuracy\",\n",
							"                             experiment_timeout_hours = 1,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = False)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# Register the best model to AML Model registry\r\n",
							"import mlflow\r\n",
							"\r\n",
							"from mlflow.models.signature import ModelSignature\r\n",
							"\r\n",
							"# Get best model from automl run\r\n",
							"best_run, onnx_model = run.get_output(return_onnx_model=True)\r\n",
							"\r\n",
							"# Infer signature of Non-ONNX model\r\n",
							"train = df.head(1)\r\n",
							"train = df.drop_column(\"tipped\")\r\n",
							"signature = None\r\n",
							"try:\r\n",
							"    signature = infer_signature(train, non_onnx_model.predict(train))\r\n",
							"except Exception as err:\r\n",
							"     print(\"Infer signature error: {0}\".format(err))\r\n",
							"\r\n",
							"artifact_path = experiment_name + '_artifact'\r\n",
							"\r\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\r\n",
							"mlflow.set_experiment(experiment_name)\r\n",
							"\r\n",
							"with mlflow.start_run() as run:\r\n",
							"    # Save the model to the outputs directory for capture\r\n",
							"    mlflow.sklearn.log_model(non_onnx_model, artifact_path, signature=signature)\r\n",
							"\r\n",
							"    # Register the model to AML model registry\r\n",
							"    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, 'AutoML-WithRegistration-20201101071229' + '_bestModel')"
						],
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkPoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkPoolMedium",
						"name": "sparkPoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkPoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"inputCollapsed": true
						},
						"source": [
							"# -------------------------------------------------------------------------------------------------\n",
							"# TODO: Remove this code cell when the latest version of the AML SDK is deployed to the base image.\n",
							"# -------------------------------------------------------------------------------------------------\n",
							"\n",
							"def parse_target(target, add_managed_dataset_prefix=False):\n",
							"    from azureml.data.azure_storage_datastore import AbstractAzureStorageDatastore\n",
							"    from azureml.data.azure_data_lake_datastore import AbstractADLSDatastore\n",
							"    from azureml.data.datapath import DataPath\n",
							"    #from azureml.data.constants import MANAGED_DATASET\n",
							"    MANAGED_DATASET = 'managed-dataset'\n",
							"\n",
							"    datastore = None\n",
							"    relative_path = None\n",
							"\n",
							"    if isinstance(target, AbstractAzureStorageDatastore) or isinstance(target, AbstractADLSDatastore):\n",
							"        datastore = target\n",
							"        relative_path = MANAGED_DATASET if add_managed_dataset_prefix else '/'\n",
							"    elif isinstance(target, DataPath):\n",
							"        datastore = target._datastore\n",
							"        relative_path = (MANAGED_DATASET if add_managed_dataset_prefix else '/') \\\n",
							"            if target.path_on_datastore is None else target.path_on_datastore\n",
							"    elif isinstance(target, tuple) and len(target) == 2:\n",
							"        datastore = target[0]\n",
							"        relative_path = target[1]\n",
							"    if not isinstance(datastore, AbstractAzureStorageDatastore) and not isinstance(datastore, AbstractADLSDatastore):\n",
							"        raise ValueError(\"The target type is not supported, target: {}\".format(target))\n",
							"\n",
							"    return datastore, relative_path\n",
							"\n",
							"\n",
							"def _set_spark_config(datastore):\n",
							"    from pyspark.sql import SparkSession\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"    spark = SparkSession.builder.getOrCreate()\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        account_name = datastore.account_name\n",
							"        account_key = datastore.account_key\n",
							"        endpoint = datastore.endpoint\n",
							"        spark.conf.set('fs.azure.account.key.{}.blob.{}'.format(account_name, endpoint), account_key)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        account_name = datastore.account_name\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        endpoint = datastore.endpoint\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.azure.account\"\n",
							"        provider = \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
							"        storage_account = \"{}.dfs.{}\".format(account_name, endpoint)\n",
							"        spark.conf.set(\"{}.auth.type.{}\".format(prefix, storage_account), \"OAuth\")\n",
							"        spark.conf.set(\"{}.oauth.provider.type.{}\".format(prefix, storage_account), provider)\n",
							"        spark.conf.set(\"{}.oauth2.client.id.{}\".format(prefix, storage_account), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.client.secret.{}\".format(prefix, storage_account), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.client.endpoint.{}\".format(prefix, storage_account),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        client_id = datastore.client_id\n",
							"        client_secret = datastore.client_secret\n",
							"        tenant_id = datastore.tenant_id\n",
							"        authority_url = datastore.authority_url\n",
							"        prefix = \"fs.adl\"  # dfs.adls deprecated\n",
							"        spark.conf.set(\"{}.oauth2.access.token.provider.type\".format(prefix), \"ClientCredential\")\n",
							"        spark.conf.set(\"{}.oauth2.client.id\".format(prefix), client_id)\n",
							"        spark.conf.set(\"{}.oauth2.credential\".format(prefix), client_secret)\n",
							"        spark.conf.set(\"{}.oauth2.refresh.url\".format(prefix),\n",
							"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"\n",
							"def _get_output_uri(datastore, path):\n",
							"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
							"\n",
							"    if datastore.datastore_type == AZURE_BLOB:\n",
							"        output_uri = 'wasbs://{}@{}.blob.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                       datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
							"        output_uri = 'abfss://{}@{}.dfs.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
							"                                                      datastore.endpoint, path)\n",
							"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
							"        output_uri = 'adl://{}.azuredatalakestore.net/{}'.format(datastore.store_name, path)\n",
							"    else:\n",
							"        raise ValueError(\n",
							"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
							"\n",
							"    return output_uri\n",
							"\n",
							"\n",
							"def write_spark_dataframe(spark_dataframe, datastore, relative_path_with_guid, show_progress):\n",
							"    console = get_progress_logger(show_progress)\n",
							"    _set_spark_config(datastore)\n",
							"    output_uri = _get_output_uri(datastore, relative_path_with_guid)\n",
							"\n",
							"    console(\"Writing spark dataframe to {}\".format(relative_path_with_guid))\n",
							"    spark_dataframe.write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save(output_uri)\n",
							"\n",
							"\n",
							"def get_progress_logger(show_progress):\n",
							"    import sys\n",
							"    console = sys.stdout\n",
							"\n",
							"    def log(message):\n",
							"        if show_progress:\n",
							"            console.write(\"{}\\n\".format(message))\n",
							"\n",
							"    return log\n",
							"\n",
							"def _check_type(arg, arg_name, expected_type):\n",
							"    if not isinstance(arg, expected_type):\n",
							"        raise ValueError(\"Expected {} of type {} but received {}\".format(arg_name, expected_type, type(arg)))\n",
							"\n",
							"def register_spark_dataframe(dataframe, target, name, description=None, tags=None, show_progress=True):\n",
							"        \"\"\"Create a dataset from spark dataframe.\n",
							"\n",
							"        :param dataframe: Required, in memory dataframe to be uploaded.\n",
							"        :type dataframe: pyspark.sql.DataFrame\n",
							"        :param target: Required, the datastore path where the dataframe parquet data will be uploaded to.\n",
							"            A guid folder will be generated under the target path to avoid conflict.\n",
							"        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
							"            or tuple(azureml.core.datastore.Datastore, str) object\n",
							"        :param name: Required, the name of the registered dataset.\n",
							"        :type name: str\n",
							"        :param description: Optional. A text description of the dataset. Defaults to None.\n",
							"        :type description: str\n",
							"        :param tags: Optional. Dictionary of key value tags to give the dataset. Defaults to None.\n",
							"        :type tags: dict[str, str]\n",
							"        :param show_progress: Optional, indicates whether to show progress of the upload in the console.\n",
							"            Defaults to be True.\n",
							"        :type show_progress: bool\n",
							"        :return: The registered dataset.\n",
							"        :rtype: azureml.data.TabularDataset\n",
							"        \"\"\"\n",
							"        from azureml.data.datapath import DataPath\n",
							"        from pyspark.sql import DataFrame\n",
							"        from uuid import uuid4\n",
							"\n",
							"        console = get_progress_logger(show_progress)\n",
							"\n",
							"        console(\"Validating arguments.\")\n",
							"        _check_type(dataframe, \"dataframe\", DataFrame)\n",
							"        _check_type(name, \"name\", str)\n",
							"        datastore, relative_path = parse_target(target, True)\n",
							"        console(\"Arguments validated.\")\n",
							"\n",
							"        guid = uuid4()\n",
							"        relative_path_with_guid = \"{}/{}\".format(relative_path, guid)\n",
							"        write_spark_dataframe(dataframe, datastore, relative_path_with_guid, show_progress)\n",
							"\n",
							"        console(\"Creating new dataset\")\n",
							"        datapath = DataPath(datastore, \"/{}/*.parquet\".format(relative_path_with_guid))\n",
							"        saved_dataset = Dataset.Tabular.from_parquet_files(datapath)\n",
							"\n",
							"        console(\"Registering new dataset\")\n",
							"        registered_dataset = saved_dataset.register(datastore.workspace, name,\n",
							"                                                    description=description,\n",
							"                                                    tags=tags,\n",
							"                                                    create_new_version=True)\n",
							"        console(\"Successfully created and registered a new dataset.\")\n",
							"\n",
							"        return registered_dataset\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"Synapse-Experiment-20201103090016\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.nyc_taxi\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
							"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"classification\",\n",
							"                             training_data = dataset_train,\n",
							"                             label_column_name = \"tipped\",\n",
							"                             primary_metric = \"accuracy\",\n",
							"                             experiment_timeout_hours = 3,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Predict NYC Taxi Tips')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkPoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkPoolMedium",
						"name": "sparkPoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkPoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load data\r\n",
							"Get a sample data of nyc yellow taxi from Azure Open Datasets"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NycTlcYellow\r\n",
							"from datetime import datetime\r\n",
							"from dateutil import parser\r\n",
							"\r\n",
							"start_date = parser.parse('2018-05-01')\r\n",
							"end_date = parser.parse('2018-05-07')\r\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
							"nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\r\n",
							"nyc_tlc_df.info()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from IPython.display import display\r\n",
							"\r\n",
							"sampled_df = nyc_tlc_df.sample(n=10000, random_state=123)\r\n",
							"display(sampled_df.head(5))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Prepare and featurize data\r\n",
							"- There are extra dimensions that are not going to be useful in the model. We just take the dimensions that we need and put them into the featurised dataframe. \r\n",
							"- There are also a bunch of outliers in the data so we need to filter them out."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import numpy\r\n",
							"import pandas\r\n",
							"\r\n",
							"def get_pickup_time(df):\r\n",
							"    pickupHour = df['pickupHour'];\r\n",
							"    if ((pickupHour >= 7) & (pickupHour <= 10)):\r\n",
							"        return 'AMRush'\r\n",
							"    elif ((pickupHour >= 11) & (pickupHour <= 15)):\r\n",
							"        return 'Afternoon'\r\n",
							"    elif ((pickupHour >= 16) & (pickupHour <= 19)):\r\n",
							"        return 'PMRush'\r\n",
							"    else:\r\n",
							"        return 'Night'\r\n",
							"\r\n",
							"featurized_df = pandas.DataFrame()\r\n",
							"featurized_df['tipped'] = (sampled_df['tipAmount'] > 0).astype('int')\r\n",
							"featurized_df['fareAmount'] = sampled_df['fareAmount'].astype('float32')\r\n",
							"featurized_df['paymentType'] = sampled_df['paymentType'].astype('int')\r\n",
							"featurized_df['passengerCount'] = sampled_df['passengerCount'].astype('int')\r\n",
							"featurized_df['tripDistance'] = sampled_df['tripDistance'].astype('float32')\r\n",
							"featurized_df['pickupHour'] = sampled_df['tpepPickupDateTime'].dt.hour.astype('int')\r\n",
							"featurized_df['tripTimeSecs'] = ((sampled_df['tpepDropoffDateTime'] - sampled_df['tpepPickupDateTime']) / numpy.timedelta64(1, 's')).astype('int')\r\n",
							"\r\n",
							"featurized_df['pickupTimeBin'] = featurized_df.apply(get_pickup_time, axis=1)\r\n",
							"featurized_df = featurized_df.drop(columns='pickupHour')\r\n",
							"\r\n",
							"display(featurized_df.head(5))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"filtered_df = featurized_df[(featurized_df.tipped >= 0) & (featurized_df.tipped <= 1)\\\r\n",
							"    & (featurized_df.fareAmount >= 1) & (featurized_df.fareAmount <= 250)\\\r\n",
							"    & (featurized_df.paymentType >= 1) & (featurized_df.paymentType <= 2)\\\r\n",
							"    & (featurized_df.passengerCount > 0) & (featurized_df.passengerCount < 8)\\\r\n",
							"    & (featurized_df.tripDistance >= 0) & (featurized_df.tripDistance <= 100)\\\r\n",
							"    & (featurized_df.tripTimeSecs >= 30) & (featurized_df.tripTimeSecs <= 7200)]\r\n",
							"\r\n",
							"filtered_df.info()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Save the data to spark table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark_df = spark.createDataFrame(filtered_df)\r\n",
							"spark_df.write.mode(\"overwrite\").saveAsTable(\"default.NYC_Taxi\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RegisterONNXModel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkPoolMedium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "python3",
						"display_name": "Python 3"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkPoolMedium",
						"name": "sparkPoolMedium",
						"type": "Spark",
						"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkPoolMedium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1603687117552
							},
							"collapsed": true
						},
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core.experiment import Experiment\n",
							"from azureml.core.workspace import Workspace\n",
							"from azureml.train.automl.run import AutoMLRun\n",
							"\n",
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"nyc_taxi_automl_run1\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment=Experiment(ws, experiment_name)\n",
							"run = AutoMLRun(experiment, run_id = \"AutoML_a0379ffb-1905-4009-817e-b2bc36806d71\")\n",
							"\n",
							"import time\n",
							"time.sleep(3600)\n",
							"\n",
							"import onnxruntime\n",
							"import mlflow\n",
							"import mlflow.onnx\n",
							"\n",
							"from mlflow.models.signature import ModelSignature\n",
							"from mlflow.types import DataType\n",
							"from mlflow.types.schema import ColSpec, Schema\n",
							"from mlflow.models.signature import infer_signature\n",
							"\n",
							"# Get best model from automl run\n",
							"best_run, onnx_model = run.get_output(return_onnx_model=True)\n",
							"\n",
							"# Define utility function to infer the schema of ONNX model\n",
							"def _infer_schema(data):\n",
							"    res = []\n",
							"    for _, col in enumerate(data):\n",
							"        t = col.type.replace('tensor(', '').replace(')', '')\n",
							"        if t in ['bool']:\n",
							"            dt = DataType.boolean\n",
							"        elif t in [\"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\"]:\n",
							"            dt = DateType.integer\n",
							"        elif t in [\"uint32\", \"int64\"]:\n",
							"            dt = DataType.long\n",
							"        elif t in [\"float16\", \"bfloat16\", \"float\"]:\n",
							"            dt = DataType.float\n",
							"        elif t in [\"double\"]:\n",
							"            dt = DataType.double\n",
							"        elif t in [\"string\"]:\n",
							"            dt = DataType.string\n",
							"        else:\n",
							"            raise Exception(\"Unsupported type: \" + t)\n",
							"        res.append(ColSpec(type=dt, name=col.name))\n",
							"    return Schema(res)\n",
							"\n",
							"def _infer_signature(onnx_model):\n",
							"    onnx_model_bytes = onnx_model.SerializeToString()\n",
							"    onnx_runtime = onnxruntime.InferenceSession(onnx_model_bytes)\n",
							"    inputs = _infer_schema(onnx_runtime.get_inputs())\n",
							"    outputs = _infer_schema(onnx_runtime.get_outputs())\n",
							"    return ModelSignature(inputs, outputs)\n",
							"\n",
							"# Infer signature of ONNX model\n",
							"signature = _infer_signature(onnx_model)\n",
							"\n",
							"artifact_path = experiment_name + '_artifact'\n",
							"\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
							"mlflow.set_experiment(experiment_name)\n",
							"\n",
							"with mlflow.start_run() as run:\n",
							"    # Save the model to the outputs directory for capture\n",
							"    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature)\n",
							"\n",
							"    # Register the model to AML model registry\n",
							"    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, 'model_name')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"gather": {
								"logged": 1603687138853
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WriteTable')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/yifso-workspace/providers/Microsoft.Synapse/workspaces/yifsoeus2euap/bigDataPools/sparkpool1",
						"name": "sparkpool1",
						"type": "Spark",
						"endpoint": "https://yifsoeus2euap.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"df = spark.createDataFrame([\n",
							"  (\"I am so happy today, its sunny!\", \"en-US\"),\n",
							"  (\"I am frustrated by this rush hour traffic\", \"en-US\"),\n",
							"  (\"The cognitive services on spark aint bad\", \"en-US\"),\n",
							"], [\"text\", \"language\"])\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"df.write.mode(\"overwrite\").saveAsTable(\"TextSentimentData\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.createDataFrame([\n",
							"    (\"1972-01-01T00:00:00Z\", 826.0, \"1\"),\n",
							"    (\"1972-02-01T00:00:00Z\", 799.0, \"1\"),\n",
							"    (\"1972-03-01T00:00:00Z\", 890.0, \"1\"),\n",
							"    (\"1972-04-01T00:00:00Z\", 900.0, \"1\"),\n",
							"    (\"1972-05-01T00:00:00Z\", 766.0, \"1\"),\n",
							"    (\"1972-06-01T00:00:00Z\", 805.0, \"1\"),\n",
							"    (\"1972-07-01T00:00:00Z\", 821.0, \"1\"),\n",
							"    (\"1972-08-01T00:00:00Z\", 20000.0, \"1\"),\n",
							"    (\"1972-09-01T00:00:00Z\", 883.0, \"1\"),\n",
							"    (\"1972-10-01T00:00:00Z\", 898.0, \"1\"),\n",
							"    (\"1972-11-01T00:00:00Z\", 957.0, \"1\"),\n",
							"    (\"1972-12-01T00:00:00Z\", 924.0, \"1\"),\n",
							"    (\"1973-01-01T00:00:00Z\", 881.0, \"1\"),\n",
							"    (\"1973-02-01T00:00:00Z\", 837.0, \"1\"),\n",
							"    (\"1973-03-01T00:00:00Z\", 9000.0, \"1\")\n",
							"], [\"timestamp\", \"value\", \"group\"])"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"df.write.mode(\"overwrite\").saveAsTable(\"AnomalyDetectionData\")"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Word_Count')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "sparkpool02",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "scala",
				"jobProperties": {
					"name": "Word_Count",
					"file": "abfss://sparkjobdefinition@automationtestadlsgen2.dfs.core.windows.net/scala/wordcount/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					},
					"args": [
						"abfss://sparkjobdefinition@automationtestadlsgen2.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjobdefinition@automationtestadlsgen2.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		}
	]
}